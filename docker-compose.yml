version: '3.8'

services:
  # Main dashboard application
  dashboard:
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATA_INGESTION_URL=http://data-ingestion:8000
      - PREPROCESSING_URL=http://preprocessing:8001
      - INFERENCE_URL=http://inference:8002
      - POSTPROCESSING_URL=http://postprocessing:8003
    volumes:
      - ./:/app
    depends_on:
      - data-ingestion
      - preprocessing
      - inference
      - postprocessing
    networks:
      - ml-inference-network

  # Data Ingestion Service
  data-ingestion:
    build:
      context: ./data_ingestion
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - PREPROCESSING_URL=http://preprocessing:8001
    volumes:
      - ./data_ingestion:/app
    networks:
      - ml-inference-network

  # Preprocessing Service
  preprocessing:
    build:
      context: ./preprocessing
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      - INFERENCE_URL=http://inference:8002
    volumes:
      - ./preprocessing:/app
    networks:
      - ml-inference-network

  # Inference Service
  inference:
    build:
      context: ./inference
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    environment:
      - POSTPROCESSING_URL=http://postprocessing:8003
      - MODEL_PATH=/app/model.pkl
    volumes:
      - ./inference:/app
    networks:
      - ml-inference-network

  # Postprocessing Service
  postprocessing:
    build:
      context: ./postprocessing
      dockerfile: Dockerfile
    ports:
      - "8003:8003"
    volumes:
      - ./postprocessing:/app
    networks:
      - ml-inference-network

networks:
  ml-inference-network:
    driver: bridge